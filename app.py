import os
os.environ["CHROMA_TELEMETRY_DISABLED"] = "true"
from dotenv import load_dotenv, find_dotenv
from zhipuLLM import ZhipuaiLLM
from zhipuEmbedding import ZhipuAiEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnableBranch

#ui
import gradio as gr


_ = load_dotenv(find_dotenv())
api_key=os.environ["ZHIPUAI_API_KEY"]

def combine_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs["context"])


def show_switch_status(switch_state):
    return switch_state

# åœ¨æ–‡ä»¶é¡¶éƒ¨å®šä¹‰è½¬æ¢å‡½æ•°
def format_chat_history(chatbot):
    """å°† Gradio Chatbot æ ¼å¼è½¬ä¸º LangChain æ”¯æŒçš„ chat_history æ ¼å¼"""
    formatted_history = []
    for human_msg, ai_msg in chatbot:
        formatted_history.append(("human", human_msg))
        formatted_history.append(("ai", ai_msg))
    return formatted_history

def chatbot_response(input, chatbot, isUseRAG):
    """æ ¹æ®å¼€å…³çŠ¶æ€è¿”å›æç¤ºä¿¡æ¯"""

    llm = ZhipuaiLLM(model_name="glm-4-plus", temperature=0.1, api_key=api_key)
    if isUseRAG:

        # é—®ç­”é“¾çš„ç³»ç»Ÿprompt
        system_prompt = (
            "ä½ æ˜¯ä¸€ä¸ªé—®ç­”ä»»åŠ¡çš„åŠ©æ‰‹ã€‚ "
            "è¯·ä½¿ç”¨æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ç‰‡æ®µå›ç­”è¿™ä¸ªé—®é¢˜ã€‚ "
            "å¦‚æœä½ ä¸çŸ¥é“ç­”æ¡ˆå°±è¯´ä¸çŸ¥é“ã€‚ "
            "è¯·ä½¿ç”¨ç®€æ´çš„è¯è¯­å›ç­”ç”¨æˆ·ã€‚"
            "\n\n"
            "{context}"
        )
        # åˆ¶å®šprompt template
        qa_prompt = ChatPromptTemplate(
            [
                ("system", system_prompt),
                ("placeholder", "{chat_history}"),
                ("human", "{input}"),
            ]
        )
         # å®šä¹‰é—®ç­”é“¾
        qa_chain = (
            RunnablePassthrough.assign(context=combine_docs) # ä½¿ç”¨ combine_docs å‡½æ•°æ•´åˆ qa_prompt ä¸­çš„ context
            | qa_prompt # é—®ç­”æ¨¡æ¿
            | llm
            | StrOutputParser() # è§„å®šè¾“å‡ºçš„æ ¼å¼ä¸º str
        )

        #
        #è·å–å¾—åˆ°å‘é‡åº“
        vectordb = Chroma(
            persist_directory='E:/ai/llm-universe/data_base/vector_db/chroma',  # å…è®¸æˆ‘ä»¬å°†persist_directoryç›®å½•ä¿å­˜åˆ°ç£ç›˜ä¸Š
            embedding_function=ZhipuAiEmbeddings()
        )
        #å–æ•°æ®
        retriever = vectordb.as_retriever(search_kwargs={"k": 1})

        
        # å‹ç¼©é—®é¢˜çš„ç³»ç»Ÿ prompt
        condense_question_system_template = (
            "è¯·æ ¹æ®èŠå¤©è®°å½•å®Œå–„ç”¨æˆ·æœ€æ–°çš„é—®é¢˜ï¼Œ"
            "å¦‚æœç”¨æˆ·æœ€æ–°çš„é—®é¢˜ä¸éœ€è¦å®Œå–„åˆ™è¿”å›ç”¨æˆ·çš„é—®é¢˜ã€‚"
            )
        # æ„é€  å‹ç¼©é—®é¢˜çš„ prompt template
        condense_question_prompt = ChatPromptTemplate([
                ("system", condense_question_system_template),
                ("placeholder", "{chat_history}"),
                ("human", "{input}"),
            ])
        
        retrieve_docs = RunnableBranch(
        # åˆ†æ”¯ 1: è‹¥èŠå¤©è®°å½•ä¸­æ²¡æœ‰ chat_history åˆ™ç›´æ¥ä½¿ç”¨ç”¨æˆ·é—®é¢˜æŸ¥è¯¢å‘é‡æ•°æ®åº“
        (lambda x: not x.get("chat_history", False), (lambda x: x["input"]) | retriever, ),
        # åˆ†æ”¯ 2 : è‹¥èŠå¤©è®°å½•ä¸­æœ‰ chat_history åˆ™å…ˆè®© llm æ ¹æ®èŠå¤©è®°å½•å®Œå–„é—®é¢˜å†æŸ¥è¯¢å‘é‡æ•°æ®åº“
        condense_question_prompt | llm | StrOutputParser() | retriever,
        )
    
        # å®šä¹‰å¸¦æœ‰å†å²è®°å½•çš„é—®ç­”é“¾
        qa_history_chain = RunnablePassthrough.assign(
            context = (lambda x: x) | retrieve_docs # å°†æŸ¥è¯¢ç»“æœå­˜ä¸º content
            ).assign(answer=qa_chain)

        result = qa_history_chain.invoke({
            "input": input,
            "chat_history": format_chat_history(chatbot)
        })

        print(result)
        chatbot.append((input,result["answer"]))
        return [chatbot,input]
    else:
        result = llm.invoke(input)
        print(chatbot)
        chatbot.append((input,result.content))
        return [chatbot,input]

# åˆ›å»ºç•Œé¢
with gr.Blocks() as demo:
    gr.Markdown("""
    # ğŸ¤– RAG æ™ºèƒ½èŠå¤©æœºå™¨äºº
    æ”¯æŒç›´æ¥è°ƒç”¨å¤§æ¨¡å‹æˆ–ç»“åˆæœ¬åœ°çŸ¥è¯†åº“ï¼ˆRAGï¼‰å›ç­”é—®é¢˜
    """)

    chatbot = gr.Chatbot(
        label="å¯¹è¯å†å²",
        height=500,  # å¯¹è¯æ¡†é«˜åº¦
        avatar_images=(None, "https://gradio.s3-us-west-2.amazonaws.com/guides/robot.png")  # ï¼ˆå¯é€‰ï¼‰ç”¨æˆ·/æœºå™¨äººå¤´åƒ
    )

    with gr.Row():
        chebox = gr.Checkbox(
            label="RAG",
            value=False
        )

    with gr.Row():
        input = gr.Textbox(
            label="è¾“å…¥ä½ çš„é—®é¢˜",
            placeholder="ä¾‹å¦‚ï¼š",
            lines=2,
            container=False
        )
        submit_btn = gr.Button("å‘é€", variant="primary", icon="ğŸ“¤")

    submit_btn.click(
        fn=chatbot_response,
        inputs=[input, chatbot, chebox],  # è¾“å…¥ï¼šç”¨æˆ·æ¶ˆæ¯ + å†å²å¯¹è¯ + å¼€å…³çŠ¶æ€
        outputs=[chatbot, input]  # è¾“å‡ºï¼šæ›´æ–°åçš„å¯¹è¯ + æ¸…ç©ºè¾“å…¥æ¡†
    )
    

if __name__ == "__main__":
    demo.launch(
        share=False,
        show_error=True,  # æ˜¾ç¤ºé”™è¯¯ä¿¡æ¯ï¼ˆè°ƒè¯•ç”¨ï¼‰
    )

    
    
     
    
