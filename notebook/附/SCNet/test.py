import os
from dotenv import load_dotenv, find_dotenv
from zhipuLLM import ZhipuaiLLM
from zhipuEmbedding import ZhipuAiEmbeddings
from langchain_core.runnables import RunnableLambda
from langchain_community.vectorstores import Chroma
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnablePassthrough, RunnableParallel
from langchain_core.output_parsers import StrOutputParser

#ui
import gradio as gr


_ = load_dotenv(find_dotenv())
api_key=os.environ["ZHIPUAI_API_KEY"]

def combine_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)

'''
def get_completion(prompt, model="glm-4-plus", temperature=1):
    messages = [{"role": "user", "content": prompt}]
    res = client.chat.completions.create(
        model=model, messages=messages, temperature=temperature
    )
    if len(res.choices) > 0:
        print(res.choices[0].message.content)
        return res.choices[0].message.content
    return "generate answer error"
'''

def show_switch_status(switch_state):
    return switch_state
    
def chatbot_response(input, chatbot, isUseRAG):
    """æ ¹æ®å¼€å…³çŠ¶æ€è¿”å›æç¤ºä¿¡æ¯"""

    llm = ZhipuaiLLM(model_name="glm-4-plus", temperature=0.1, api_key=api_key)
    if isUseRAG:

        #è·å–å¾—åˆ°å‘é‡åº“
        vectordb = Chroma(
            persist_directory='E:/ai/llm-universe/data_base/vector_db/testchroma',  # å…è®¸æˆ‘ä»¬å°†persist_directoryç›®å½•ä¿å­˜åˆ°ç£ç›˜ä¸Š
            embedding_function=ZhipuAiEmbeddings()
        )
        #å–æ•°æ®
        retriever = vectordb.as_retriever(search_kwargs={"k": 2})

        #åˆå¹¶æ•°æ®
        combiner = RunnableLambda(combine_docs)
        retrieval_chain = retriever | combiner

        #æ„å»ºé“¾
        template = """ä½¿ç”¨ä»¥ä¸‹ä¸Šä¸‹æ–‡æ¥å›ç­”æœ€åçš„é—®é¢˜ã€‚å¦‚æœä½ ä¸çŸ¥é“ç­”æ¡ˆï¼Œå°±è¯´ä½ ä¸çŸ¥é“ï¼Œä¸è¦è¯•å›¾ç¼–é€ ç­”
        æ¡ˆã€‚æœ€å¤šä½¿ç”¨ä¸‰å¥è¯ã€‚å°½é‡ä½¿ç­”æ¡ˆç®€æ˜æ‰¼è¦ã€‚è¯·ä½ åœ¨å›ç­”çš„æœ€åè¯´â€œè°¢è°¢ä½ çš„æé—®ï¼â€ã€‚
        {context}
        é—®é¢˜: {input}
        """
        # å°†templateé€šè¿‡ PromptTemplate è½¬ä¸ºå¯ä»¥åœ¨LCELä¸­ä½¿ç”¨çš„ç±»å‹
        prompt = PromptTemplate(template=template)

        qa_chain = (
            RunnableParallel(  {"context": retrieval_chain, "input": RunnablePassthrough()})
            | prompt
            | llm
            | StrOutputParser()
        )

        result = qa_chain.invoke(input)
        print(result)
        chatbot.append((input,result))
        return [chatbot,input]
    else:
        result = llm.invoke(input)
        print(chatbot)
        chatbot.append((input,result.content))
        return [chatbot,input]

# åˆ›å»ºç•Œé¢
with gr.Blocks() as demo:
    gr.Markdown("""
    # ğŸ¤– RAG æ™ºèƒ½èŠå¤©æœºå™¨äºº
    æ”¯æŒç›´æ¥è°ƒç”¨å¤§æ¨¡å‹æˆ–ç»“åˆæœ¬åœ°çŸ¥è¯†åº“ï¼ˆRAGï¼‰å›ç­”é—®é¢˜
    """)

    chatbot = gr.Chatbot(
        label="å¯¹è¯å†å²",
        height=500,  # å¯¹è¯æ¡†é«˜åº¦
        bubble_full_width=False,  # æ°”æ³¡ä¸å æ»¡å®½åº¦ï¼ˆæ›´ç¾è§‚ï¼‰
        avatar_images=(None, "https://gradio.s3-us-west-2.amazonaws.com/guides/robot.png")  # ï¼ˆå¯é€‰ï¼‰ç”¨æˆ·/æœºå™¨äººå¤´åƒ
    )

    with gr.Row():
        chebox = gr.Checkbox(
            label="RAG",
            value=False
        )

    with gr.Row():
        input = gr.Textbox(
            label="è¾“å…¥ä½ çš„é—®é¢˜",
            placeholder="ä¾‹å¦‚ï¼š",
            lines=2,
            container=False
        )
        submit_btn = gr.Button("å‘é€", variant="primary", icon="ğŸ“¤")

    submit_btn.click(
        fn=chatbot_response,
        inputs=[input, chatbot, chebox],  # è¾“å…¥ï¼šç”¨æˆ·æ¶ˆæ¯ + å†å²å¯¹è¯ + å¼€å…³çŠ¶æ€
        outputs=[chatbot, input]  # è¾“å‡ºï¼šæ›´æ–°åçš„å¯¹è¯ + æ¸…ç©ºè¾“å…¥æ¡†
    )
    

if __name__ == "__main__":
    demo.launch(
        server_port=7861,
        server_name="127.0.0.1",
        share=False,
        show_error=True  # æ˜¾ç¤ºé”™è¯¯ä¿¡æ¯ï¼ˆè°ƒè¯•ç”¨ï¼‰
    )

    
    
     
    
